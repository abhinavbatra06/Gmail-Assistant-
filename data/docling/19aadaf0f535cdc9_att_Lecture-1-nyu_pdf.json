{
  "filename": "Lecture-1-nyu.pdf",
  "path": "data/attachments/19aadaf0f535cdc9_Lecture-1-nyu.pdf",
  "text": "[CSCI-GA 3033-091] Fall 2025 Introduction to LLM based Generative AI Systems\nLecture 1   09/03/24\n1\nParijat Dube <pd2637@columbia.edu>\nChen Wang <cw3687@columbia.edu>\nClass Introduction\n‚Ä¢\nInstructors\nParijat Dube <pd2637@columbia.edu>\nAdjunct Professor, CS Dept\nMachine learning, deep learning, System performance optimization Generative AI for enterprise automation\nChen Wang <cw3687@columbia.edu>\nAdjunct Professor, CS Dept\nKubernetes, Container Cloud Platform, Data-driven/QoE based resource management, AI4Sys & Sys4AI, LLM Serving & Finetuning Sustainable Cloud & AI systems\n2\nClass Introduction\nCourse Assistants:\nAneesh Mokashi\nakm9999@nyu.edu\nGeetha Guruji gg3039@nyu.edu\nPrerequisites:\nKnowledge of ML and use of ML algorithms\nProgramming  in Python\nClass on Brightspace:\nhttps://brightspace.nyu.edu/d2l/home/504821\nAll information about the class will be available here including syllabus, announcements and assignments\n3\nToday's Agenda\nCourse Overview\nSyllabus\nAssignments and Grading\nLogistics\nMachine Learning Systems\nMachine Learning on Cloud and Model Lifecycle\nModel Performance and Complexity Tradeoffs\nClass 1 Topics\n4\nCourse Information\n¬∑ What this course will cover ?\nDL concepts, training architectures, hyperparameters\nLLM pre-training, fine-tuning and inference serving systems\nCloud based DL/LLM systems and performance issues\nLLM systems performance evaluation tools, techniques, benchmarks\nLLM systems performance optimization\nProgramming assignments involving GPUs on cloud\nResearch paper readings\n¬∑ What this course will not cover ?\nOther DL architectures like CNN, GANs, Autoencoders etc.\nMathematical analysis of DL algorithms\n5\nEducational Objectives\nIdentify different components of DL /LLM system stack and their interdependencies\nKnowledge of  ML/LLM model lifecycle and steps in making a trained model production ready\nTrain a DL/LLM model and make it a web service for inferencing\nPerformance considerations, tools, techniques at different stages of model lifecycle: development, testing, deployment.\nDL/LLM training pitfalls and techniques/best practices data processing\nAbility to train DL/LLM models on cloud platforms using GPUs\nPerformance characterization of DL/LLM systems\nKnowledge of DL/LLM benchmarks and performance metrics\nPerformance optimization of DL/LLM systems\n6\nClass 1: Fundamentals of Deep Learning (DL)\nML performance concepts/techniques: overfitting, generalization, bias, variance tradeoff, regularization\nPerformance metrics: algorithmic and system Level\nDL training hyperparameters\nbatch size, learning rate, momentum, weight decay\nSingle node vs distributed training\nModel and Data Parallelism\nParameter server, all reduce\nConvergence and runtime\nHardware Acceleration: GPUs, TPUs, NCCL\n7\nClass 2: Attention, Transformer, and Popular Large Language Models (LLMs)\nSeq2Seq models\nEncoder and decoder\nAttention mechanism\nTransformer architecture: self-attention, multi-head attention, encoderdecoder attention\nLLMs: BERT, OpenAI GPT, LLAMA, Gemini, Claude, IBM Granite\n8\nClass 3: Cloud Technologies and ML Platforms\nML System Stack on Cloud\nThis class introduces the ML system stack on cloud platforms, focusing on:\nMicroservices architecture as foundation Major cloud ML platforms Distributed training frameworks GenAI application development tools\nMicroservices Architecture\nDocker\nContainerization for consistent environments\nkubernetes\nKubernetes\nContainer orchestration for scalable deployment\nCloud ML Platforms\nAWS SageMaker\nIBM Watsonx\nDistributed Training & ML Pipelines\nago RAY\nRay\nFramework for distributed computing\n(Optional) TorchX for PyTorch-based distributed training Scalable ML workflows across clusters Efficient resource management\nGenAI Application Tools\nDify\nLLMOps platform for AI applications\nGradio\nRapid UI creation for ML models\n& gradio\nEnable rapid, often no-code prototyping of GenAI demos\nBuilding Full-Stack GenAI Systems\nPractical insights into end-to-end ML systems Integration of microservices, cloud platforms, and GenAI tools Focus on Google Cloud Vertex AI for course projects Hands-on experience with scalable deployment\nAzure ML Studio\nGoogle Vertex AI\n(Focus for course projects)\nClass 4: Prompt Engineering and LLM Apps\n‚Ä¢ Zero-shot Prompting\n‚Ä¢ Few-shot Prompting\n‚Ä¢ Chain-of-Thought,\nAutomatic CoT\n‚Ä¢\n... ...\n‚Ä¢ LangChain\n‚Ä¢ LlamaIndex\n‚Ä¢ Basics of\nPrompting\n‚Ä¢ Prompt Elements\n‚Ä¢ General Tips\n‚Ä¢ Examples\n‚Ä¢ Translation\n‚Ä¢ Code Generation\n‚Ä¢ Summarization\n‚Ä¢ Proofread and Correct\n‚Ä¢ Math Calculation\n‚Ä¢ Entity Extraction\n‚Ä¢ Call Functions\nLLM Use\nCases\nPrompt\nEngineering\nPrompt\nEngineering\nTechniques\nLLM App\nDevelopment\n10\nClass 5: RAG and LLM Agents\nCapabilities & Limitations of LLM\nKnowledge Cutoffs\nHallucinations\nStructured Data Challenge\nBiases\nRetrieval Augmented Generation\nUse Cases\nSemantic Search\nSummarization\nKeyword Search and Embeddings\nRetrieval and Rerank\nAnswer Generation\nVector Databases\n11\nClass 6: Pre-Training for LLM\nPre-training Concepts\n1.Training from existing foundation models\n2.Training from scratch\n3.Model selection from HuggingFace and PyTorch hubs\nTraining Process for Different Architectures\n1.Encoder-only models\n2.Decoder-only models\n3.Sequence-tosequence (seqto-seq) models\nManaging High Memory Requirements\n1.Quantization techniques\n2.Challenges with consumer-grade hardware\nScaling Model Training\n1.Distributed Data Parallel (DDP)\n2.Fully Sharded Data Parallel (FSDP)\n3.Zero Redundancy Optimizer (ZeRO)\nOptimizing Training Resources\n1.Balancing model size, training data volume, and compute budget\n2.Insights from the Chinchilla study\nUse Cases for Custom LLM Pretraining\n1.Domain adaptation (e.g., law, medicine)\n2.Introduction to BloombergGPT as an example\n12\nClass 7: Fine Tuning Techniques\nInstruction Fine-\nTuning, Prompt\nTuning and\nPEFT\nInstruction Fine-Tuning Process\n‚Ä¢\nTraining Data Preparation\n‚Ä¢\nDataset Splitting\n‚Ä¢\nPrompt-Completion Pairs\n‚Ä¢\nPerformance Evaluation\nMulti-task Fine-\nTuning\n‚Ä¢\nBenefits\n‚Ä¢ FLAN Models\nPEFT\n‚Ä¢\nIntroduction of PEFT\n‚Ä¢\nBenefits\n‚Ä¢ PEFT Methods\nLoRA\n‚Ä¢\nIntroduction of LoRA\n‚Ä¢\nBenefits\n‚Ä¢ Performance\nPrompt Tuning\n‚Ä¢\nExplanation\n‚Ä¢\nBenefits\nComparison and Practical\nApplication\n‚Ä¢ LoRA vs Prompt Tuning\n‚Ä¢ QLoRA\n‚Ä¢\nBenefits\n13\nClass 8: Efficient Serving of LLMs\n14\nLLM Serving\nResource\nOptimization\nBatching\n:\nStatic Batching\nDynamic Batching\nContinuous\nBatching\nMemory\nOptimization\nFlash Attention\nPaged Attention\nKernel\nUnified Paging\nLLM Serving\nFrameworks\nvLLM\nDeepspeed-MII\nTensorRT\nHuggingFace TGI\nServer\nFine-Tuned Model\nServing\nvLLM\nS-LoRA\nLoRAX\nPerformance &\nTrade-offs\nPerformance vs.\nThroughput\nFairness in\nScheduling/Routing\nGPU Techniques\nGPU Sharing\nGPU Optimization\nMultiplexing\nClass 9: RLHF\n15\nTool Learning Framework\nInstruction\nResponse\nClass 10: Tool-assisted LLMs and Agentic AI\nTool Set\nLLM\nHistory\nFeedback\nExecution\nPlanning\n16\nScheduled daily\nTool to get current location of user\nTool to send API request forecast for the day of a\nand get the weather locauoni\nLLM\nTool to send email\nEmail sent if unideal\nweather\nEmail content is tips for unideal weather conditions\nClass 10: Agentic systems\nWeather-Advisory Agent Architecture\n(1d) ReAct (Reason + Act)\nThought 1: I need to search Apple Remote and find the program it was originally designed to interact with.\nAct 1: Search[Apple Remote]\nObs 1: The Apple Remote is a remote control introduced in ront Row media cente cy designed to control the\nThought 2: Apple Remote was originally designed to control the Front Row media center program. I need to search\nFront\nRow next and find what other device can control it.\nAct 2: Search[Front Row]\nObs 2: Could not find [Front Row]. Similar: ['Front Row\nSeat to Earth', 'Front Row Motorsports',' Front Row\n(software) ', ...]\nThought 3: Front Row is not found. I need to search Front\nRow (software) .\nAct 3: Search[Front Row (software) j\nObs 3: Front Row is a discontinued media center software ...\nThought 4: Front Row (software) is controlled by an Apple\nRemote or the keyboard function keys. So the answer is keyboard function keys. Act 4: Finish [keyboard function keys ]\n17\nCommunication\nChannels\nMessoge\n4-MCP Protocol-\nMCP Host\n(Claude, IDEs, Tools)\nYour Computer\nMultiagent system Architecture\nLocall\nMCP Protocol\nWet Server Al\nResource Al\nCollaboration Stratergy\nTask\nInformation\nAllocation\nMCP Server B\nI Agent\nAgent 1\nAgent 3\nAgent 2 4\nMCP Protocol\nMCP Server C|\nQuery\nEnvironment\nPhysical\nEnvironment\nClass 11: Multi-agent System & MCP\nKnowledge Base\nSharding\nLocal\nKnowiedge\nSharing\nResource B\nKnowiedge\nSharing\nInternet\nRemote\nResource C\nMulti-agent Systems\nA pool of specialized agents collaborating to solve complex tasks\nBenefits: modularity, specialization, and control in agentic AI systems\nWe'll study connection patterns between agents\nFrameworks: LangGraph and CrewAI\nMulti-Context Protocol (MCP)\nFramework enabling LLMs to efficiently manage multiple conversations\nImproves performance and reduces computational costs\nWe'll explore MCP clients and servers development\nPractical implementations for optimizing LLM interactions\nAdvanced AI Frameworks for Collaborative Intelligence\n-Web APIs.\n–≠6\nClass 12: Multimodal Generative AI systems\nDefinition & Importance\nDefinition : Process multiple data types (text, images, audio, video).\nImportance : Human-like perception and interaction.\nBeyond Language Models\nExpansion : Incorporate visual, auditory, sensory inputs.\nExamples : DALLE, GPT-4 with vision.\nCreating Large Multimodal Models (LMMs)\nIncorporation : Additional modalities into LLMs.\nArchitectures : Encoder-decoder, transformer-based.\nTraining : Crossmodal, contrastive learning.\nFlamingo : Visual language model, few-shot learning.\nThe Multimodality Revolution\nShift : From unimodal to multimodal AI.\nAdvancements : Computer vision, speech recognition, NLP.\nIntegration : Unified models.\n19\nClass 12: Emerging Topics: Multimodal Generative AI\nVision-Language Models & Voice LLM | 21-11-2025\nMultimodal AI & LMMs\nHands-on Project\nClass 12: Emerging Topics: Multimodal Generative AI\n‚Ä¢ VLMs: Vision-Language Models for visual reasoning\nVision-Language Models & Voice LLM | 21-11-2025\nMultimodal AI & LMMs\nBeyond LLMs:\nProcessing multiple data types simultaneously\nLMMs:\nSpecialized encoders with shared reasoning layers\nVLMs: Vision-Language Models for visual reasoning\nApplications:\nHealthcare, retail, robotics, education\nAl Voice Agents\nDefinition: Systems for real-time conversations\n*o Architecture: STT ‚Üí NLU ‚Üí LLM ‚Üí TTS pipeline\n% Latency: Critical for natural conversation flow\nAI Voice Agents\n\"The future of Al is multimodal, integrating multiple senses like human intelligence.\"\nDefinition: Systems for real-time conversations\nArchitecture:\nSTT ‚Üí NLU ‚Üí LLM ‚Üí TTS pipeline\nLatency: Critical for natural conversation flow\nHands-on Project\nGoal: Build production-ready voice agent\nPartners:\nLiveKit, RealAvatar, ElevenLabs\nLearning:\nPipeline, optimization, deployment\nFuture Directions\nPretraining\nStage2:Multi-task\nUnified Models:\nSingle architectures for all modalities\nEmbodied AI:\nIntegration with physical interaction\nLearnable\nQuery\nEmbs\nLeamnable\nOvaryl\nEmbs\n\"The future of AI is multimodal, integrating multiple senses like human intelligence.\"\nViT\nImage-Text Pairs\nVIT\nMulti-task and\nInterleaved VL Data\nViT\nChat Interleaved\nVL. Data\nThe rating age line the One a nong: (cle Bo, Tet he go an end Ma. 8: 2306. 7294, (\nC 8 Thou 1 12022) Owen.V ‚Ä¢ A Vercatilel\nCrossAttr\nQuery\nStage3: Supervised\nFinetuning\nQwenLM\nClass 13: LLM Benchmarks\nLLM Benchmarks\n21\nPurpose &\nMotivation\n:\nPurpose\nEvaluate\nperformance,\nEfficiency &\nLimitations.\nMotivations\nAccuracy,\nspeed,\nreliability.\nTypes of\nBenchmarking\nModel\nPerformance,\naccuracy\nmetrics.\nSystem\nEfficiency\nScalability\nResources\n.\nEvaluation\nMetrics\nROUGE\nBLEU\nN-grams\nPrecision/Re\ncall/F1\nAdvanced\nTechniques\nComprehensive\nBenchmarks\n:\nGLUE\nSuperGLUE\nMMLU\nBIG-\nbench\nHELM\nUnseen Data\nEvaluation\nGeneraliz\nation\nRisks\nTools\nMLPerf\nLLMPerf\nHuggingFace\nLeaderboard\nFmperf\nRecommended Books\nThis course does not follow any  textbook\nBackground knowledge\nList of books (covering deep learning topics)\nCharu Aggarwal 'Neural Networks and Deep Learning',  available at https://link.springer.com/book/10.1007/978-3-319-94463-0\nGoodfellow, Bengio, Courville, 'Deep Learning', available at http://www.deeplearningbook.org\nThese books are good for basic DL understanding\nFor basics of machine learning concepts an excellent textbook is G. James et al ' Introduction to Statistical Learning Theory '. Second version available is available for free download at https://www.statlearning.com\nAll other reading material will be posted on Canvas.\n22\nAssignments and Grading\n¬∑ Distribution of marks :\nAssignments: 40%\nQuizzes: 20%\nFinal Project: 40%\n¬∑ Assignments (40%)\n5 assignments\nAssignments posted at the end of lectures 2, 4, 6, 8, 10; due in 2 weeks\nAll programming assignments should be done as Jupyter notebooks, unless specified otherwise\n¬∑ Quizzes (20%)\nCanvas\n5 quizzes\n23\nAssignments and Grading (contd.)\nFinal Project (40%): Team assignment. Team of 2. Any project involving development of new LLM solutions and/or performance optimization of existing LLM systems. 2-page project proposal due by Midterm. Detailed rubric shall be provided. Project grading:\nProject proposal (5%) - due in mid October\nMidpoint checkpoint (5%)  - due before Thanksgiving break\nGithub repo with README, documented code (5%)\nFinal presentation and demo (15%)\n24\nClass Logistics\nReach CAs: Office hours and Campuswire\nAccess to Computer Clusters\nClass communications:Campuswire\n25\n/A.I. TIMELINE\n1950\n1955\nTURING TEST\nComputer scientist\nAlan Turing proposes a\nA.I. BORN\nTerm 'artificial\n1961\nUNIMATE\nFirst industrial robot,\n1964\nELIZA\nPioneering chatbot intelligence' is coined\nUnimate, goes to work intelligence\ndeveloped by Joseph\nAI  Timeline\n1999\nAIBO\nSony launches first consumer robot pet dog autonomous robotic\nAiBO (Al robot) with skills and personality\nthat develop over time\nInflection point in AI adoption is tied to innovations in computing and availability of big data Jeopardy comments number (2170) of\nCloud computing was coined\nO: AlphaGo\n2002\n2011\nROOMBA\nSIRI\nFirst mass produced\nApple integrates Siri,\nan intelligent virtual\nvacuum cleaner from assistant with a voice iRobot learns to navigate interface, into the and clean homes iPhone 4S\n2011\nWATSON\nIBM's question\nanswering computer\nWatson wins first place on popular $1M prize television quiz show Turing Test with a third of judges believing Eugene is human\n2014\nEUGENE\nEugene Goostman, a\nchatbot passes the\n2014\nALEXA\nAmazon launches Alexa.\nan intelligent virtual\nassistant with a voice interface that completes shopping tasks media making inflammatory and offensive racist\n2016\nTAY\nMicrosoft's chatbot Tay\ngoes rogue on social\n2017\nALPHAGO\nGoogle's A.l. AlphaGo\nbeats world champion\nKe Jie in the complex\nboard game of Go, notable for its vast\n2006: Amazon elastic cloud and S3 was launched\n1966\nSHAKEY\nThe 'first electronic person' from Stanford,\nA.I.\nWINTER\nMany false starts and\n1997\nDEE.\nDeep Blue, a chess- playing computer from\n1998\nKISMET\nCynthia Breazeal at MIT\nintroduces KISmet, an\nSIZIGY\n26\nFactors Contributing to AI Success\n¬∑ Algorithms, Data, Compute, Applications\nDistributed training algorithms scaling upto 100s of GPUs\nData growing at exponential rate; Internet, Social media, Internet of thngs (IoT)\nCompute power growth with specialized cores; GPUs, TPUs\nDevelopment of innovative applications\n2012 Alexnet by Krizhevsky et al at ImageNet Competition\nSimple convolutional neural network: 5 convolutional, 3 fully connected\nGPU based; Beat other models by 11% margin\nTriggered 'Cambrian Explosion' in deep learning technologies\n' Neural networks are growing and evolving at an extraordinary rate, at a lightening rate,‚Ä¶.What started out just five years ago with AlexNet‚Ä¶five years later, thousands of species of AI have emerged .'\n27\nTransformers\nArchitecture\nAug 2017\nGPT-2\n1.5B Param\nFeb 2019\nMed-PaLM2\nMEENA\n2.6B Param\nJan 2020\nLLaMA\nBERT\n480B Param\nNov 2021\n‚Ä¢\nAtlas\nGopher\n280B Param\nDec 2021\n–°–ú3\nEvolution of Large Language Models (LLMs)\nJourney\nContinues\nBloombergGPT\nMed-PaLM\nZ-Code++\nPaLM\n340B Param\n50B Param\nApr 2023\nMar 2023\nDec 2022\n710M Param\nAug 2022\nApr 2022\nGPT-4\n540B Param\nFigure from Mohamadi et al, ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey\n‚Ä¢\nLaMDA\n137B Parm\nJun 2021\n‚Ä¢\n28\nAI Blogs of Major Companies\nMeta: https://ai.meta.com/blog/\nGoogle: https://ai.googleblog.com\nIBM Research:\nhttps://www.ibm.com/blogs/research/category/ai/\nMicrosoft: https://news.microsoft.com/source/topics/ai/\nAWS Machine Learning Blog: https://aws.amazon.com/blogs/machine-learning/\n29\nMachine Learning System\nA composition of one or more software components, with possible interactions, deployed on a hardware platform with the purpose of achieving some performance objective.\nA Machine Learning system is a system where  one or more software components are machine learning based .\nWhy study ML systems ?\nAlgorithms run on real and (possibly) faulty hardware in production environments\nTheoretical performance is far away from observed\nTo characterize holistic performance of not just the algorithm but the end-to-end performance of the entire system\n30\nConstituents of a ML System\nMachine Learning\nSystem\nData\nAlgorithm(s)\nSoftware Platform\nHardware/\nInfrastructure\nModel\n31\nInfrastructure\nCompute units and accelerators, memory, storage, network\nResources can be acquired as bare metal, VMs/Containers on cloud\nDesign better hardware\nAdapt existing architectures to ML tasks.\nDevelop brand-new architectures for ML.\nHardware compute precision affects performance: tradeoff between accuracy and runtime\n32\n(Learning) Algorithm\nGeneral and domain specific architectures\nHyperparameter tuning to extract the best performance\nEffects the resource requirements: compute (FLOPS), memory\nPerformance (runtime) and scalability of an algorithm depends on:\nHardware/Infrastructure\nSoftware platform (frameworks, libraries, drivers)\n33\nData\nData as a critical element; Data is the king in ML\nDifferent modalities: Audio, video, images, text\nData sources, collection, labeling, quality, data storage\nData type determines the choice of learning algorithm\nMaking the data business ready is challenging\nMany data-driven organizations are spending 80 percent of their time on data preparation and find it a major bottleneck.\nDataOps: tools, processes, and organizational structures to cope with significant increase in volume, velocity, and variety of data.\n34\nEnd-to-end traceability\nPLAN\nCONTINOUS\nINTEGRATION\nCODE\nFEEDBACK\nCONTINUOUS\nSoftware Engineering in ML Systems\nMachine learning applications run as pipelines that ingest data, compute features,  identify model(s), discover hyperparameters, train model(s), validate and deploy model(s).\nMaking a model as a production-capable web service\nContainerization (docker), cluster deployment (K8s)\nAPIs exposed as web service (Tensorflow serving/ONNX runtime)\nWorkflow engines (e.g., Kubeflow) automate the ML pipeline\nDeployment monitoring and operational analytics\nDevops principles applicable to ML Systems:\nContinuous Integration, Continuous delivery (CI/CD)\nPredictability\n'A model may be unexplainable-but an API cannot be unpredictable'\nReproducibility and Traceability\nProvenance for machine learning artifacts\nML specific testing and monitoring apart from traditional software testing\nData testing\nInfrastructure testing\nModel testing\nProduction testing\nDEPLOY\n35\nInhibitors in Successful Implementation of ML Solutions\nDeployment and automation\nReproducibility of models and predictions\nDiagnostics\nGovernance and regulatory compliance\nScalability\nCollaboration\nMonitoring and management\n36\nCloud Computing\nAccess to computing resources and storage on demand\nPay-as-you go model\nHeterogeneous resources: GPUs, CPUs, storage type\nDifferent offering models: IaaS, PaaS, SaaS, MLaaS\nDifferent deployment models: Public, private, hybrid cloud\nProvisioning, maintenance, monitoring, life-cycle-management\n37\nCLOUD\nCloud and AI\nComputer\nAI\nHarness power of Big Data and compute\nTablet computer\nWearable device\nCloud\nUnmanned truck\nAccess to Big Data\nPlatform to quickly develop, deploy, and test AI solutions\nEase in AI reachability\nIOT\nAL\nHouse\nRobot\nPet\nYuichi Yoda\n38\nCloud based Machine Learning Services\nIBM Watson Studio\nhttps://www.ibm.com/products/watson-studio\nAmazon Sagemaker\nhttps://aws.amazon.com/sagemaker\n‚Ä¢\nMicrosoft Azure Machine Learning\nhttps://azure.com/ml\n‚Ä¢\nGoogle Vertex AI Platform\nhttps://cloud.google.com/vertex-ai/\n39\nLayer 8: Observability & Management Layer miflow\nOpenTelemetry\nMLflow\nOpenLLMetry\nMonitoring\nLayer 7: Orchestration & Application Layer\nGenAI on Cloud Stack\nKServe\nVLLM\nNVIDIA Triton\nEnvoy AI\nLayer 5: Fine-Tuning & Development Layer\nAxoloti\nLayer 4: Data & Model Management Layer\nModel Catalog\nLayer 3: Cloud-Native Storage Layer\nPortworx\nLayer 2: Orchestration Layer\nKubernetes\nLayer 1: Cloud Hardware & Accelerated Compute\nHigh-Performance CPUs\nUnsloth\nTorchtune\nHugging Face\nTools and frameworks for customizing pre-trained models\nPEFT\nCentralized management of models, data, and metadata\nVector DBs\nPostgreSQL\nMongoDB\nData Lakes\nPersistent, scalable storage for models, data, and artifacts\nRWX Volumes\nPersistent Volumes\nManages and orchestrates hardware resources for GenAl workloads\nSlurm\nGPU Operator\nContainer Toolkit\nProvides the raw computational power required for GenAl workloads\nSpecialized GPUs\nHigh-Speed Storage\nHigh-Bandwidth Networking\nGenAI on Cloud Full Stack\nMonitoring, management, and governance across the entire stack\nBuilding complex GenAI applications and workflows\nHigh-performance model serving and inference optimization\n40\nAccuracy\nMonitoring\nModel\nServing\nModel\nHardening\nParameter\nTuning\nAI Model Training Lifecycle\nData\nFeature\nPreprocessing\nEngineering\nModel Training\nLifecycle\nPerformance considerations at each stage\nData preprocessing: de-noising, de-biasing, train/test set creation\nFeature engineering: search efficient data transformations\nModel training: model identification/synthesis, hyperparameter tuning, regularization\nModel hardening: efficient adversarial training\nModel serving:  hardware, model pruning and compression\nMonitoring: response time, drift detection\nContinuous learning: model adaptability, retraining\n41\nConfiguration\nData Collection\nServing\nPractical Machine Learning Systems\nFeature\n‚úî\n‚úî\n‚úî\n‚úî\n‚úî\n‚úî\n‚úî\nExtraction\nManagement Tools\nProcess\nData\nVerification\nMachine\nResource\nManagement\nMonitoring\n42\nClass 1: Fundamentals of Deep Learning (DL)\n43\nTSS = [(yi -y)?\nTSS - RSS\nTSS\nf=B +Bx1 + B2x2+...\nR? =\nRSS =\nMiles per gallon\n06 006\nLinear Regression\n+ –í—Ä—Ö—Ä.\nResidual Plot for Linear Fit\n3230\n0334\n3300\n308\n100\n150\nHorsepower\n200\nRSS\nTSS\nDegree 2\nDegree 5\n50\ni=1\n= 1 -\n5\n10\n8\n155¬∞\n15\n20\nFitted values\n25\n15\n20\n30 35\n25\nFitted values\n8\n9899\nResidual Plot for Quadratic Fit\n0334\n3230\n44\n√ù = f(X)\nMSE ==\nn\n20\n40\n60\n80\nChOL 1\n2.0\n- IlVi - f(xi)) 3\ni=1\n100\n2\n5\n10\n20\n20\n40\n60\n80\n100\n2\nMean Square Error (MSE)\n1\nn\nCASE 1\nCASE 2\nCASE 3\ntrue value\npredicted\n‚Ç¨(n.112\nMean Squared Error\n1.5\n1.0\nCASE 2\n2.0\nMean Squared Error\n10\nFlexibility\n20\n20\n40\n60\n80\nLASE S\nMean Squared Error\n100\n5\n10\nFlexibility\n20\n45\nOverfiYng and UnderfiYng\n20\n80\n100\nOverfi]ng: model performs well on training data but does not generalize well to unseen data (test data)\nUnderfi]ng: model is not complex enough to capture pa^ern in the training data well and therefore suffers from low performance on unseen data\n46\nThe Bias-Variance Trade-Off\nOPTIMAL\nCOMPLEXITY\nOVERALL ERROR\nSQUARED ERROR\n‚Ä¢ Optimal point of model complexity is somewhere in middle.\nVARIANCE,\nMODEL COMPLEXITY\n47\n2.5\n2.0\n1.5\n2\nE (y0 - f(xo))\n2.5\n2.0\n1.5\nModel Complexity Tradeofffs\nCASE 1\nCASE 2\nCASE 3\n5\nT\n10\nFlexibility\n20\n2\n5\nT\n10\nFlexibility\n20\n2\n5\n10\nFlexibility\n20\n= Var (f (xo)) + [Bias(f (xo))]¬™ + Var(c)\nCHOL 1\nChOL <\n15\nMSE\nBias\nVar\n48\nPrediction Error\nHigh Bias\nLow Variance\nSimple model\nFail to completely capture the relaZonship between features\nIntroduces bias: Consistent test error across different choices of training data\nLow variance\nIncreasing training data does not help in reducing bias\nComplex model captures nuances in training data causing Overfieng\nLow bias\nTrain error << Test error\nWith different training instances, the model predicZon for same test instance will be very different - High Variance\nLow Bias\nHigh Variance\n.....--/\nModel Complexity Tradeoffs\nGap\nGeneraliza>on\nGap\n49\nLow Bias\nHigh Bias\nLow Variance\nHigh Variance\nBias-Variance Tradeoff\nHigh Bias\nLow Bias\nIncrease model complexity\nHigh Variance\nIncrease training data\nLow Variance\n50\nRegularizaEon\nTechniques used to improve generalizaSon of a model by reducing its complexity\nTechniques to make a model perform well on test data oTen at expense of its performance on training data\nAvoid overfiVng, reduce variance\nSimpler models are preferable: low memory, increase interpretability\nHowever simpler models may reduce the expressive power of models\nSample techniques:\nParameter norm penalKes\nùë≥ ùüê and ùë≥ ùüè norm weight decay\nNoise injecKon\nDropout\n51\n(LASSO)\n400\nn\nn\nStandardized Coefficients\nStandardized Coefficients\n300\n‚Ä¢ ' –£—ñ-–í–æ-!\n200\ni=1\ni=1\n100\n-100\n-300\n1e-02\n400\n300\n200\n100\n-200\n20\n50\n100\n200\n500\n2\n2\n+1 21811\n400\nj=1\n100 200 300\nj=1\n-100\nStandardized Coefficients\nRegulariza_on in Regression\nùë≥ ùüê Regularization Loss (Ridge Regression) 300 400 ... ...... ¬∑\nLimit\nRating\nStudent\n2000\n5000\nùë≥\n0.0\n0.2\nWhat value of lambda to choose ?\n1.0\nIncome\nLimit\nRating\nStudent\nBixig )\n(1-80-284))\n0.4\n0.6\n0.8\n52\n52\nMean Squared Error\n60\n50\n40\n30\n20\n10\n60\n50\n40\n30\n20\nBias-variance tradeoff with Lambda\n10-01\n1e+03\n0.0\n0.2\n0.4\n0.6\n|BR 12/1181|2\n0.8\n1.0\n53\nPerformance Metrics\nAlgorithmic performance: accuracy, precision, recall, F1-score, ROC,\nSystem performance: training ame, inference ame, training cost, memory requirement, training efficiency\n54\nPredicted value\nAccuracy =\ntp + tn tp+tn+ fp+fn\nPrecision =\ntp tp + fp\nFalse discovery rate = 1-Precision\nRecall =\ntp + fn\nTrue negative rate =\ntn tn + fP\nAccuracy, Precision, Recall, Specificity\nfalse negative (fn)\nPositive\nNegative\nNegative\nPositive\ntrue positive (tp)\ntrue negative (tn)\nfalse positive (fp)\nTrue value\nBalanced accuracy =\n(ùëÜùëíùëõùë†ùëñùë°ùëñùë£ùëñùë°ùë¶ + ùëÜùëùùëíùëêùëñùëìùëñùëêùëñùë°ùë¶)/2\nConsiders all entries in the confusion matrix Value lies between 0 (worst classifier) and 1 (best classifier)\n55\nF1 =\n2\nprecision ‚Ä¢ recall recall-1 + precision -1\nFp = (1 + B¬™).\n‚Ä¢ EB\nprecision ‚Ä¢ recall precision + recall\ntp tp + ‚Öñ(fp + fn)\n= 2.\n√ü ‚Ä¢ precision + recall score: B = 1 is F1 score; recall is considered\nBalancing Precision and Recall\nF1 score: Harmonic mean of precision and recall; measure of classifier accuracy\n¬∑ ùë†ùëêùëúùëüùëí: \tùõΩ = 1\tùëñùë†\tùêπ1\tùë†ùëêùëúùëüùëí ; recall is considered Œ≤ ames as important as precision\n=\n56\n1.0-\n0.8\n0.6 g\n0.4\n0.2 1\n0.0.0\nTrue Positive Rate\nROC Curves\nReceiver Opera_ng Characteris_cs (ROC)\nPlots true posiave rate (Recall) vs false posiave rate at different thresholds\n57\n‚Ä¢ DL performance is closely tied to the hardware\n‚Ä¢ compute power, memory, network\n‚Ä¢ Tesla V100: 640 tensor cores (> 100 TFLOPS), 16 GB\n‚Ä¢ NVIDIA NVLink: 300 GB/s\nDL Training Time\nDeep Learning Training in Less Than a Workday\nDL performance is closely aed to the hardware\ncompute power, memory, network\nTesla V100: 640 tensor cores (> 100 TFLOPS), 16 GB\nNVIDIA NVLink: 300 GB/s\nVolta opimized CUDA libraries\nPASCAL\nVOLTA TENSOR CORES\n58\n58\n47X Higher Throughput Than CPU Server on Deep\nLearning Inference\nTesla V100\nTesla P100\n15X\nDL Inference Throughput\nPerformance Normalized to CPU\nWorkload: ResNet-50 | CPU: 1X Xeon E5-2690v4 @ 2.6 GHz | GPU: Add 1X Tesla P100 or V100\n47X\n59\nX.\nN\n–•–∑\nOUT PUT LAYER\n‚Üí –£\nDeep Learning Training\nForward phase\nLoss calculaion\nBackward phase\nWeight update\nForward Phase\nBackward Phase\nCompute Loss\nINPUT LAYER\nHIDDEN LAYER\nL.\n60\nDeep Learning Training Steps\n¬∑ Forward phase:\ncompute the acKvaKons of the hidden units based on the current value of weights\ncalculate output\ncalculate loss funcKon\n¬∑ Backward phase:\ncompute parKal derivaKve of loss funcKon w.r.t. all the weights;\nuse backpropaga8on algorithm to calculate the parKal derivaKves recursively\nbackpropagaKon changes the weights (and biases) in a network to decrease the loss\nUpdate the weights using gradient descent\n61\nStochas_c Gradient Descent (SGD)\nLoss is calculated using one training data at each weight update\nStochastic gradient descent is only a randomized approximation of the true loss function.\n62\nHyperparameters in Deep Learning\nNetwork architecture: number of hidden layers, number of hidden units per later\nAcavaaon funcaons\nWeight iniaalizer\nLearning rate\nBatch size\nMomentum\nOpamizer\n63\nCost\nLearning step\nRandom\nCost\nStart\nStart\n‚Ä¢ 0\nLearning rate: large value vs small value\nconverge to cost minima and the cost might keep increasing with\nfurther training loops.\ntraining loops to arrive at cost minima.\nImage Credit : \"Hands-on Machine Learning with Scikit-Learn and TensorFlow\" by Aurelien Geron\nMinimum\nCost\nBatch size\nEffect of batch size on learning\nBatch size is restricted by the GPU memory (12GB for K40, 16GB for P100 and V100) and the model size\nModel and batch of data needs to remain in GPU memory for one iteraKon\nBatch size tradeoffs\nHardware constraints (GPU memory) dictate the largest batch size\nShould we try to work with the largest possible batch size ?\nLarge batch size gives more confidence in gradient esKmaKon\nLarge batch size allows you to work with higher learning rates, faster convergence\nLarge batch size leads to poor generalizaSon\nLands on sharp minima wheareas small batch SGD find flat minima which generalize beTer\nSmall batches introduce stochasKc noise into gradient esKmates, which acts as implicit regularizaKon by helping the opKmizer escape sharp local minima\n65\nSingle Node, Single GPU Training\nTraining throughput depends on:\nNeural network model (acKvaKons, parameters, compute operaKons)\nBatch size\nCompute hardware: GPU type (e.g., Nvidia M60, K80, P100, V100)\nFloaKng point precision (FP32 vs FP16)\nUsing FP16 can reduce training Rmes and enable larger batch sizes/models without significantly impacRng the accuracy of the trained model\nIncreasing batch size increases throughput\nBatch size is restricted by GPU memory\nTraining Sme with single GPU very large: 6 days with Places dataset (2.5M images) using Alexnet on a single K40.\nSmall batch size => noisier approximaSon of the gradient => lower learning rate => slower convergence\n--QPI\n‚Ä¢ PCI-e switch\n-- PCI-e O GPU\n‚Äî NVLink ¬© CPU\nCO\nX-Bus\nGOO\nNVLink-V1\nGO\nG1,\n‚Ä¢\nG1\nSPI\nNVLink-V2\n063\n√ìG2\nG2\n(A) SummitDev\nNVLink\nG40\nG3\nG7\nCO\nX-Bus\nGO\nG1\nG1,\nNVLink\nG2\nC1\nO.\nG4\nG3\n(B) Summit\nG4 Q\nOPI\nG3\n‚Ä¢ G7\nFig. 2: NVLink interconnect topology for SummitDev and Summit.\nG5\nPci-e\nG6\nG6\nPCI-e\nMul5-GPU Execu5on Scaling\nCuolunting Madorn CDIl Intarcannant.\nVeracal scaling-up in a single node\nNVIDIA DGX-1 (8 P100 GPUs) and DGX-2 (16 V100 GPUs) servers\nHorizontal scaling-out across multiple nodes\nExample: GPU accelerated supercomputers like Summit and Sierra from US Department of Energy\nEvalua[ng Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect\nG5\nG2\nGO\nSingle Node, Mul_ GPU Training\nCommunication libraries (e.g., NCCL) and supported communication algorithms/collectives (broadcast, all-reduce, gather)\nNCCL ('Nickel') is library of accelerated collectives that is easily integrated and topology-aware so as to improve the scalability of multi-GPU applications\nCommunication link bandwidth: PCIe/QPI or NVlink\nCommunication algorithms depend on the communication topology (ring, hub-spoke, fully connected) between the GPUs.\nHigh Performance Networking\nLarge Scale Parallel and Deep Learning applicaaons needs:\nHigh Bandwidth\nLow Latency\nEthernet is not enough\nInfiniband (IB) is widely adopted\nCustom Networks are the best\n69\nCPU\nGPU.\nPCle: Peripheral Component Interconnect express\nSMP: symmetric multiprocessing\nQPI: Quick-path interconnect (25.6 GB/s)\nGPU3\nRing based collectives\nGPUO\nSMP Connection\n(e.g., QPI)\nPCIe: Peripheral Component Interconnect express\nSMP: symmetric multiprocessing\nQPI: Quick-path interconnect  (25.6 GB/s)\nPCIe: Peripheral Component Interconnect express\nSMP: symmetric multiprocessing\nQPI: Quick-path interconnect  (25.6 GB/s)\nCPU\nSwit sh\nCPU\nwitch\nGPUZ\nGPU3\nGPU4\n5\nGPUS\nGPU7\n1‚Ç¨\nDistributed Training\nType of Parallelism: Model, Data, Hybrid\nType of AggregaSon: Centralized, decentralized\nCentralized aggregaSon: parameter server\nDecentralized aggregaSon: P2P, all reduce\nPerformance metric: Scaling efficiency\nDefined as the raKo between the run Kme of one iteraKon on a single GPU and the run Kme of one iteraKon when distributed over n GPUs.\nParallelism\nParallel execuaon of a training job on different compute units through scale-up (single node, mulaple and faster GPUs) or scale-out (mulaple nodes distributed training)\nEnables working with large models by paraaoning model across learners\nEnables efficient training with large datasets using large 'effecave' batch sizes (batch split across learners)\nSpeeds up computaion\nModel, Data, Hybrid Parallelism\nen lanet la Blande\nLearner 2\nModel Parallelism\n¬∑ Splitting the model across multiple learners\nLearner 1\nLearner 3\nLearner 2\nLearner 4\n5 layered neural network\nPartitioned across 4 learners\nBold edges cross learn boundaries and involve inter-learner communication\nPerformance benefits depend on\nConnectivity structure\nCompute demand of operations\nHeavy compute and local connectivity -benefit most\nEach machine handles a subset of computation\nLow network traffic\nDevice 3\nDevice 2\nDevice 1\nDevice 0\nLoss\n–í–≥\n(b)\nGPipe Pipelining\nGradients\n(a)\n–í2.3\nB.\nB3.1\nB22\nB3.0\nB2.1\nB.\nB2.0\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nF1,0 F1.1 F12 F1.3\nBubble\nFoo For Foz Fo,s\n(c)\nB1.3\nB1.2\nB1.1\n–í–æ–∑ Bo2\nB1,0\nBo, 1\nBo,0\nMILI\nUpdate\nUpdate\nGpipe: a pipeline parallelism open-source library that allows scaling any network that can be expressed as a sequence of layers.\nSplit global batch into multiple micro-batches and injects them into the pipeline concurrently\nNot memory-friendly and will not scale well with large batch. The activations produced by forward tasks have to be kept for all micro-batches until corresponding backward tasks start, thus leads to the memory demand to be proportional (O(M)) to the number of concurrently scheduled micro-batches (M).\nFo\nFo\n1 1\nFo\nB.\nTime\n–í.\nHPML\nGPipe: Easy Scaling with Micro-Batch Pipeline Parallelism\n74\nModel Parallelism and model saving technique in Amazon Sagemaker model parallel library\nPipeline Parallelism\nTensor Parallelism\nOptimizer state sharding\nActivation offloading and checkpointing\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-intro.html\nPartition 0\nPartition 1\nL1\nL2\nL3\nL4\nSample model with four layers\nBatch 1\nBatch 2\nmicrobatch 1\nmicrobatch 2\nmicrobatch 3\nmicrobatch 4\nL1\nL2\nGPU 0\nL3\nGPU 1\nMP_GROUP=PP_GROUP\nmicrobatch 1\n\"microbatches\": 4,\n\"pipeline_parallel _degree\": 2,\n\"ddp\": True,\nModel parallel configuration\nBatch 3\nBatch 4\nmicrobatch 1\nmicrobatch 1\nPipeline Parallelism\nDP_GROUP\nL2\nL1\nL2\nL1\nL2\nGPU 4\nGPU 6\nL4\nL3\nL4\nL3\nL4\nGPU 5\nGPU 7\nL1\nGPU 2\nL3\nGPU 3\nML instance with eight GPU workers\nfour-way data parallelism\ntwo-way pipeline parallelism\nEach model replica is a PP_GROUP ( pipeline parallel group ) and is partitioned across two GPUs\nEach partition of the model is assigned to four GPUs, where the four partition replicas are in a data parallel group and labeled as DP_GROUP\nL1\nBatch 1\nL2\nDP_GROUP\nTensor Parallelism\nGPU O\nMP GROUP=TP_ GROUP\nL1\nL2\nGPU 1\nBatch 2\nGPU 2\nGPU 4\n\"tensor_parallel_\n_degree\": 2,\n\"pipeline_parallel_degree\": 1,\n\"ddp\": True\nModel parallel configuration\nBatch 5\nBatch 7\nL1\n....\nL2\nL3\nL4\nGPU 6\nL3\nL4\nL1\nL2\nL3\nGPU 3\nL4\nBatch 4\nL1\nL2\nL3\nGPU 5\nL4\nL1\nL2\nL3\nGPU 7\nBatch 6\nBatch 8\nL1\nL2\nL3\n-\nSample model with four layers\nBatch 3\nSplits individual layers, or nn.Modules , across devices, to be run in parallel\ntwo-way tensor parallelism\ndegree of data parallelism is eight\nL1\nL1\nOptimizer\nState for L1\nL2\nGPU O\nL2\nL3\nGPU 2\nOptimizer\nState for L3\nL1\nL2\nL3\nSample model with four layers\nOptimizer for L2\ndon't need to replicat a single replica of the\nno redundancy acros:\nTimeline on GPU 0\nduring a backward propagation\nOp5mizer state sharding\nNetwork\nR\nReduce collective\nL2\nL3\nGPU 3\nOptimizer\nState for L4\nL4\nL1\nL4\nAG\ndon't need to replicate your optimizer state in all of your GPUs a single replica of the optimizer state is sharded across data-parallel ranks, with no redundancy across devices\nState\nAll-gather collective\nModel\nReplicas\nData\nShards\nData Parallelism\nLearner 1\nLearner 2\nLearner 3\nModel is replicated on different learners\nData is sharded and each learner work on a different pariion\nHelps in efficient training with large amount of data\nParameters (weights, biases, gradients) from different replicas need to be synchronized\nModel\nReplicas\nData\nParameter server (PS) based Synchroniza_on Shards\n‚àÜùë§\n!,#\n‚àÜùë§\n!,$\nùë§\n!%$\nùë§\n!%$\nParameter Server\nEach learner executes the enSre model\nATer each mini-batch processing a learner calculates the gradient and sends it to the parameter server\nThe parameter server calculates new value of weights and sends them to the model replicas\nSynchronous SGD and the Straggler problem\nPS needs to wait for updated gradients from all the learners before calculaang the model parameters\nEven though size of mini-batch processed by each learner is same, updates from different learners may be available at different ames at the PS\nRandomness in compute ime at learners\nRandomness in communicaion ime between learners and PS\nWaiang for slow and straggling learners diminishes the speed-up offered by parallelizing the training\nPS\nFully Sync-SGD\nWo\nWI\nK-sync SGD\nWo WI\nPS\nK-batch-sync SGD\nps e\nW1.\nW2\nSynchronous SGD Variants\nP: total number of learners\nK: number of learners/minibatches the PS waits for before updating parameters\nLightly shaded arrows indicate straggling gradient computations that are canceled.\nK-sync SGD: PS waits for gradients from K learners before updating parameters; the remaining learners are canceled\nWhen K = P , K-sync SGD is same as Fully Sync-SGD\nK-batch sync: PS waits for gradients from K mini-batches before updating parameters; the remaining (unfinished) learners are canceled\nIrrespective of which learner the gradients come from\nWherever any learner finishes, it pushes its gradient to the PS, fetches current parameter at PS and starts computing gradient on the next mini-batch based on the same local value of the parameters\nRuntime per iteration reduces with K-batch sync; error convergence is same as K-sync\nAsynchronous SGD and Stale Gradients\nPS updates happen without waiang for all learners\nWeights that a learner uses to evaluate gradients may be old values of the parameters at PS\nParameter server asynchronously updates weights\nBy the ime learner gets back to the PS to submit gradient, the weights may have already been updated at the PS (by other learners)\nGradients returned by this learner are stale (i.e., were evaluated at an older version of the model)\nStale gradients can make SGD unstable, slowdown convergence, cause sub-opamal convergence (compared to Sync-SGD)\nWorker 1\nWorker 2\nParam. Version\nWor\nWos\n100\n101\n102\nWiol\nW102\n103\n104\n105\n106\nW/01= W/00 - 24W,7\nWoz= Wor - 24W,s\nStale Gradient Problem in Async-SGD\nWorker 3\nWorker 4\nW100\nW99\nW103\nW104\nblog.skymind.ai Distrbuted Deep Learning, Part 1: An IntroducJon to Distributed Training of Neural Networks\nTime\nblog.skymind.ai Distrbuted Deep Learning, Part 1:\nIntroduction to Distributed Training of Neural Netw\nPS\nAsync SGD\nWo W,W,W3\nK-async SGD\nWo\nPS\nW1\nW3\nK-batch-async SGD\nWo\nW1\n2\nW3\nW\nPS\nAsynchronous SGD and Variants\nK-asvne SGD: PS waits for cradients from K learners before undating narameters but the remaining learners are not canceled: e\nK-async SGD: PS waits for gradients from K learners before upda>ng parameters but the remaining learners are not canceled; each learner may be giving a gradient calculated at stale version of the parameters\nWhen K = 1 , K-async SGD is same as Async-SGD\nK-batch async: PS waits for gradients from K mini-batches before updating parameters; the remaining learners are not canceled\nWherever any learner finishes, it pushes its gradient to the PS, fetches current parameter at PS and starts computing gradient on the next mini-batch based on the current value of the PS\nRun>me per itera>on reduces with K-batch async; error convergence is same as K-async\n7.5\n5.0\n‚Ä¢ 2.5\n0 -2.5\n-7.5\nsame learnina rate\n- - -\nK-Batch Async\nK-Async SGD\nConvergence-Runtime Tradeoff in SGD Variants K=3 K=4\n2\n3\n4\n5x 105\n‚Ä¢ Error-runtime trade-off for Sync and Async-SGD with\nError-runtime trade-off for Sync and Async-SGD with same learning rate.\nAsync-SGD has faster decay with time but a higher error floor.\nTime\n‚Ä¢ -‚Ä¢ Fully Sync\nWall clock time\nK=4 Batch Sync\nDuOa et al. Slow and stale gradients can win the race: errorrunJme tradeoffs in distributed SGD. 2018\nSynchronous\nAsync SGD\nReduc_on over Gradients\nTo synchronize gradients of N learners, a reduction operation needs to be performed\n87\nSend\nGPU 2\nGPU 4\nReceive\nReceive\nGPU 1\nGPU 0\nSend\nReduc_on Topologies\nSend\nGPU 4\nReceive\nParameter server: single reducer\nGPU 2\nSUM (Reduce operation) performed at PS\nReceive\nSend\nReceive\nGPU 3\nGPU 1\nReceive\nSend\nGPU O\nSend\nGPUs arranged in a logical Ring (aka bucket) : all are reducers\nSUM (Reduce operation) performed at all nodes\nEach node has a left neighbor and a right neighbor\nNode only sends data to its right neighbor, and only receives data from its left neighbor\n88\nGPU O\nGPU 0\nGPU 1\nGPU 1\nGPU 2\nGPU 2\nGPU 3\nGPU 3\nGPU 4\nGPU 4\nArrays Being Summed b2+b,+bg+be+bo CatCz+CatCptC, datdg+dotd,+de eptente, teztez\na, +apta,+az+a4\nao a, taptaz+a3+a4\nCo ba+b,+bg+ba+bo Cz+G2+G4+Co+C, de+dg+do+d,+d2 estente, +e2+–µ3\nay a, taptaz+az+a4\nba+b,+bg+ba+bo Cz+C2+C4+Co+C, de+dg+do+d,+d2 estente,+e2+e3\n22\nb2\nAll-Reduce\nb4\na, taptartaz+a4\nb2+b,+b3+b4+bo G3+C2+C4+Go+G,\nPartitioning of an array into N chunks\nFinal state after all allgather transfers\nde+dz+do+d,+d2 eptente,+e2+e3\n89\nGPU O\nGPU 1\nGPU 2\nRing All-Reduce\nGPU 3\nGPU 4\n–°–∑\n¬∑ Two step algorithm:\nda\nScatter-reduce\nGPUs exchange data such that every GPU ends up with a chunk of the final result\nAllgather\nGPUs exchange chunks from scatter-reduce such that all GPUs end up with the complete final result.\nArrays Being Summed do\nd,\n90\nGPU 0\nGPU O\nGPU O\nGPU O\nGPU 1\nGPU 1\nGPU 1\nGPU 1\nGPU 2\nGPU 2\nGPU 2\nGPU 2\nGPU 3\nGPU 3\nGPU 3\nGPU 3\nGPU 4\nGPU 4\nArrays Being Summed\nArrays Being Summed bo\nbo\nCo bo\nCo do\nCz+G2+C4+G0\nda+dy+do bo\nCo dy+dy+do\nb, b,\nb, b2\nbz+b, b2+b,\nb,+b, b,+b, +b3\nC,\nC,\nC,\nC,\nC2\nC2\nC3+62\ng3+62\naol ao\nay a, +ao\na, +ao a, +ao\na, tapta,\n22\na, +a, +a2\nag a, tapta,+ag\nGPU 4\ndo d,\nd, dy\nda+dz+do+d, de\nde dg\ndg b,+b, +b3\nC3 +62\nRing All-Reduce: Scacer-Reduce Step\nGPU 4\n¬Æ+¬Æ4\neg+e4\n¬©o+e4\neg+eg+e, e,\ne, eptente, +e2\n–õ\n91\nGPU 0\nGPU 1\na, +ao ba+b,+b3+b4+bo\nb, da+da+do\nda+dg+do+dy\nRing All-Reduce: End of Scacer-Reduce Step\nb2+b, +b3\nGPU 3\nGPU 4\na, tag ta, +ag\n03+02\na, tap+az+az+a\nb2+b, +b3+b4\nC3+62+64\nFinal state after all scatter-reduce transfers\n‚Ç¨4\nCz+C2+64+60\nHow many iterations in scatter-reduce step with N GPUs ?\n92\nGPU\nSend\nGPU 0\nChunk 1\nReceive b2+b,+b3+b4+bo\nChunk O\nGPU 1\na, +ao\nChunk 2 Chunk 1\nRing All-Reduce: What to do next ?\nChunk 4 Chunk 3\nGPU 3\nGPU 4\na, tag ta, +ag ba+b,+b3\nC3+62\neptente, tez+e3\na, tap+az+az+a4\nba+b, +b3+ba\nCz+C2+64\nFinal state after all scatter-reduce transfers\n‚Ç¨4\nChunk 0 Chunk 4\nb,\n1\n2\n3\n4\nCz+C2+04+60\nda+da+do da+da+do+d,\neo+e4\neotente,\n93\nGPU 0\nGPU 0\nGPU 1\nGPU 1\nGPU 2\nGPU 2\nGPU 3\nGPU 3\nGPU 4\nGPU 4\nGPU 0\nGPU O\nGPU 1\nGPU 1\nGPU 2\nGPU 2\nGPU 3\nGPU 3\nGPU 4\nGPU 4\na, tao taz+az+a4\na, +ao a, +ao\na, +ao+a2\na, +ap+a2\nb2+b, +bz+b,+bo b2+b,+b3+bg+bo\nb2+b, +b3+b,+bo b,\nb2+b, bz+b,\nd,+dz+do da+dy+do\nda+dg+do+d, dy+dy+do+d,\nds+dg+do+d, +d2\nde+d+do+d, +d2\neo+e4\n¬Æ+‚Ç¥4\neoteq+e, eptente,\neptez+e, +e2\nCz+62+C4+Co\nC3+C2+C4+C0\nC3+02+G4+60+61\nC3+62+C,+Go+C,\nCz+C2+C4+Go+C,\nRing All-Reduce: AllGather Step\na, tap+az+az+a4\na, +ao+az+ag+a4\nb2+b, +b3+b4\nbz+b, +bg+ba\nCz+C2+C4\nd, +d, eptente, +e2+e3\nCz+C2+C4\na, +ag+az+az+a,\nb2+b, +bg+b,+bo\nC3+C,+C,+C0\nde+dg+do+d, +d2\nepte,+e, +e2+@3\na, tap+az+az+a4\nbz+b, +bg+b, +bo\nde+dz+do+d,\neptente, te2+e3\na, +ap+a, +a+a4\nbz+b, +bg+be+bo\nC+C2+C, +Go+C,\nde+dg+do+d, +d2\n@ote4+e, +e2\na, tag+a2+a3\nb2+b, +bg+b,+bo\nCz+62+C4+Go+C,\ndy+dg+do+d, +d2\neptente, tez+ez\na, taptaztag+a4\nb2+b, +b3+04\nde+dg+do+d, +d2\neptente, +ez+e3\na, +ao+a2+ag+a4\nbz+b, +b3+b4+bo\nCz+C2+C4+Co\nd,+dy+do\neotente, +e2+e3\na, tap+az+az+a4\nbatb,+b3+ba+bo\nC3+C2+C4+G0+C,\nda+dg+do+d,\neote4+e,\na, tao+a2\nb2+b, +b3+b4+bo\nC3+62+G4+C0+C,\nds+dz+do+d, +d2\neotente, +e2\na, +ao+a2+az\nbz+b, +b3\nCz+G2+G4+Gg+C,\nds+dy+do+d, +d2\neptente, +e2+e3\na, +ap+a,+–∞–∑+–∞–¥\nb2+b, +b3+b4\nC3+C2 +C4\nds+dz+do+d,+d2\n@p+@4+@, +@2+83\n94\nGPU 0\nGPU 1\nGPU 2\na, taptaz+az+a4\nb2+b,+b3+b4+bo a,taptaz+az+ay ba+b,+ba+ba+bo\nC3+62+64+60+61\nCz+G2+G4+Gp+C1\nd4+dg+do+d,+d2 ¬Æ+@4+@,+@2+¬Æ3\ndg+dg+do+dy+d2 @otente,+e2+e3\nRing All-Reduce: End of AllGather Step a,taptaz+ag+a4 ba+b,+bg+ba+bo Cz+G2+G4+G0+C1 ds+dg+do+d,+d2 eptente,+e2+e3\nGPU 3\nGPU 4\nCz+G2+C4+G+C, d+dg+d,+d,+d2¬Æ+¬©,+@,+@2+¬Æ3\na, taptaz+az+ad ba+b,+bg+ba+bo\na,taptaz+az+a4\nba+b,+b3+ba+bo Gz+C2+C4+Cp+C, da+dg+do+d,+d2 estente, +e2+e3\nFinal state after all allgather transfers\nHow many iterations in allgather step with N GPUs ?\n95\nParameter Server (PS) vs Ring All-Reduce: Communica_on Cost\nP: number of processes  N: total number of model parameters\nPS (centralized reduce)\nAmount of data sent to PS by (P-1) learner processes: N(P-1)\nAfter reduce, PS sends back updated parameters to each learner\nAmount of data sent by PS to learners: N(P-1)\nTotal communication cost at PS process is proportional to 2N(P-1)\nRing All-Reduce (decentralized reduce)\nScatter-reduce: Each process sends N/P amount of data to (P-1) learners\nTotal amount sent (per process): N(P-1)/P\nAllGather: Each process again sends N/P amount of data to (P-1) learners\nTotal communication cost per process is 2N(P-1)/P\nPS communication cost is proportional to P whereas ring all-reduce cost is practically independent of P for large P (ratio (P-1)/P tends to 1 for large P)\nWhich scheme is more bandwidth efficient ?\nNote that both PS and Ring all-reduce involve synchronous parameter updates\n96\nAll-Reduce applied to Deep Learning\nBackpropagaaon computes gradients starang from the output layer and moving towards in the input layer\nGradients for output layers are available earlier than inner layers\nStart all reduce on the output layer parameters while other gradients are being computed\nOverlay of communicaaon and local compute\n97\nDistributed Deep Learning Benchmarking Methodology\nSpeedup\nScaling efficiency\nAccuracy and end-to-end training ame\nNeural network\nDeep learning framework\nGPU type\nCommunicaaon overhead\nSpeedup (throughput) with n machines = n x Scaling efficiency with n machines\n98\nScaling efficiency\nScaling efficiency: ratio between the run time of one iteration on a single GPU and the run time of one iteration when distributed over N GPUs. Why is this ratio a measure of scaling efficiency ?\nOne can satisfy any given scaling efficiency for any neural network by increasing the batch size and reducing communication overhead\nToo big a batch size will result in converging to an unacceptable accuracy or no convergence at all\nA high scaling efficiency without being backed up by convergence to a good accuracy and end to end training time is meaningless\n99\nImagenet1K/ResNet50 Training at Scale\nCho et al achieved highest scaling efficiency; Goyal et al and Ying et al achieved highest accuracy\n100\nmultiple rings in horizontal and vertical orientations.\nRING Vo\nRING HO\n2-D Torus Topology for inter-gpu communication\nmultiple rings in horizontal and vertical orientations.\n(O,Y-1) -\nRING VI\nRING Vx-1\n(X-1,Y-1)\n101\nI. Reduce-Scatter in the horizontal direction\nAGPUO\n2\n3\n10\n11\n4\n12\n5\n13\nAdd\nAdd\nGPUT\n6\n7\nGPU32\n14\n15\n8\n16\n6\n22\nII. All-Reduce in the vertical direction\nGPUO\n8\n3\nAdd\nGPU2\n24\n11\n2-D Torus all-reduce\nIII. All-Gather in the horizontal direction\n4\n5\n12\n13\nIV. Completed\n2D-Torus all-reduce steps of a 4-GPU cluster, arranged in 2x2 grid\n28\n28\n32\n3\nGPU2\n32\n11\n4\n12\n5\n6\n36\nGPU3\n40\n28\n32 36\n13\n14\n36\n40\n28\nGPU2\n32\n36\n40\n28\nGPUI\n6\n10\nAdd\nGPU3\n14\n26\n12\n28\nGPU1\n32 36 40\nGPU3\n40\n36\n40\nhttps://arxiv.org/pdf/\nhttps://arxiv.org/pdf/1811.05233.pdf\n28\n32\n1\n9\nCopy\nImages per second\n160000\n140000\n120000\n100000\n80000\nHigh\nBandwidth\n60000\nMemory\n40000\n20000\nTensor Processing Units (TPUs)\nNumber of TPU v2 chios\nTPU:  application specific integrated circuits to accelerate machine learning workload\nTPU pod: multiple TPU chips connected to each other over a dedicated high-speed network connection\nhttps://cloud.google.com/tpu/docs/systemarchitecture\nTPU v2 pod for ResNet-50: linearly scalable\nNumber of TPU v2 chips\n*\nObserved\nPerfect\nTensorCore\nScalar Unit\nVector Unit\nMatrix\nMultiplication\nUnit\nMatrix\nMultiplication\nUnit\nTPU v5e chip\nA TensorCore has four matrix-multiply units (MXUs), a vector unit, and a scalar unit https://cloud.google.com/tpu/docs/v5e\n103\n$100.00\n$75.00\n$50.00\n$25.00\n$0.00\nResNet-50 Training Cost Comparison\n8 V100 GPUs\n27 times faster training\nTPUs vs GPUs Performance\n1 full Cloud TPU v2\nPod training duration:\n7.9 minutes for 90 epochs\n256 TPUs v\n256 TPUs v2 chips\nFull Cloud TPU v2 Pod\nGoogle Cloud VM with 8 100 GPUs\n104\n1-Chip VM\nChipo\nChip1\nTPU host in a v5e\n4-Chip VM\nChip4\nChip5\nTPU VM on Google Cloud\nChip2\nChip3\nChipo\nChip7\nCPUO\n'Cle\nNUMA O\nA TPU host is a VM that runs on a physical computer connected to TPU hardware. TPU workloads can use one or more host.\nTutorial on using Google Cloud TPU VM: Google Cloud Quickstart\nCPU1\nPC/e\nNCCL\nNvidia Common Communications Library\nNCCL implements optimized multi-GPU and multi-node communication primitives for NVIDIA gpus and networking\nNCCL provides routines such as all-gather, all-reduce, broadcast, reduce, reduce-scatter as well as point-to-point send and receive\nCommunication primitive are optimized to achieve high bandwidth and low latency over PCIe and NVLink high-speed interconnects within a node and over NVIDIA Mellanox Network across nodes\nPrepare for Lecture 2\nAccess to compute cluster\nSet up you  NYU HPC access; instructions coming soon\nFirst home-work posted on 09/12\n107\nBackup material\n108\nRapid evolution of new models\nGPT-4o : text, audio, images\nClaude 3.5 Sonnet\nLLaMa 3\nDalle 2\nGemini 1.5\n109\nAI at US Open\nhttps://www.ibm.com/case-studies/usopen\nGenerative AI models for generating content\nIBM Granite foundation models\nIBM watsonX AI and data platform built for business\nwatsonX.data: to connect and curate the USTA's trusted data sources\nFoundation models were trained to translate tennis data into cogent descriptions\nsummarizing entire matches in the case of Match Reports\ngenerating sentences that describe the action in highlight reels for AI Commentary\n'Foundation models are incredibly powerful and are ushering in a new age of generative AI, But to generate meaningful business outcomes, they need to be trained on high-quality data and develop domain expertise. And that's why an organization's proprietary data is the key differentiator when it comes to AI.'\nShannon Miller, IBM Consulting\n110\nof a model\nExample: Predict y from x\nLINEAR SIMPLIFICATION\nTRUE MODEL\nBias of a model\nx=2\nx\n‚Ä¢ First impression: Polynomial model such as y = wot wat\nW2x? + w323 + wax is \"better\" than linear model y = wo +\nW1X.\n-\n111\nDifferent Training Data Sets with Five Points\nLINEAR SIMPLIFICATION\nTRUE MODEL\nx: = 2\nSAMPLE FIVE TRAINING POINTS\nx=2\nSAMPLE FIVE TRAINING POINTS\nSAMPLE FIVE TRAINING POINTS\nx=2\nLINEAR PREDICTION AT x=2\n*=2 POLYNOMIAL PREDICTION AT *=2\nx=2\n‚Ä¢ Zero error on training data but wildly varying predictions of\n112\nObservations\n‚Ä¢ The higher-order model is more complex than the linear\nmodel and has less bias.\n- But it has more parameters.\nFor a small training data set, the learned parameters will\nbe more sensitive to the nuances of that data set.\n- Different training data sets will provide different predic-\ntions for y at a particular x.\n- This variation is referred to as model variance.\n‚Ä¢ Neural networks are inherently low-bias and high-variance\nlearners ‚Üí Need ways of handling complexity.\n113\nBias-Variance Trade-off: Setup\n‚Ä¢ Imagine you are given the true distribution B of training data\n(including labels).\nYou have a principled way of sampling data sets D ~ B from the training distribution.\nImagine you create an infinite number of training data sets (and trained models) by repeated sampling.\nYou have a fixed set T of unlabeled test instances. - The test set T does not change over different training\ndata sets.\n- Compute prediction of each instance in T for each trained\nmodel.\n114\nInformal Definition of Bias\n‚Ä¢ Compute averaged prediction of each test instance a over different training models g(x, D).\n‚Ä¢ Averaged prediction of test instance will be different from\ntrue (unknown) model f(x).\n‚Ä¢ Difference between (averaged) 9(x, D) and f(x) caused by\nerroneous assumptions/simplifications in modeling ‚Üí Bias\n- Example: Linear simplification to polynomial model causes bias.\nIf the true (unknown) model f(x) were an order-4 poly-\nnomial, and we used any polynomial of order-4 or greater in g(x, D), bias would be 0.\n115\nInformal Definition of Variance\n‚Ä¢ The value g(x, D) will vary with D for fixed x.\n- The prediction of the same test instance will be different\nover different trained models.\nAll these predictions cannot be simultaneously correct ‚Üí\nVariation contributes to error\n‚Ä¢ Variance of g(x, D) over different training data sets ‚Üí Model\nVariance\nExample: Linear model will have low variance.\nHigher-order model will have high variance.\n116\nBias-Variance Equation\n‚Ä¢ Let E[MSE] be the expected mean-squared error of the fixed\nset of test instances over different samples of training data\nsets.\nE[MSE] = Bias? + Variance + Noise (1) - In linear models, the bias component will contribute more to E[MSE].\ntribute more to E[MSE].\n- In polynomial models, the variance component will con-\nWe have a trade-off, when it comes to choosing model complexity!\n117\nLearning rate and Batch size relationship\n'Noise scale' in stochastic gradient descent (Smith et al 2017)\nThere is an optimum noise scale g which maximizes the test set accuracy (at constant learning rate)\nIntroduces an optimal batch size proportional to the learning rate when B ‚â™ N\nIncreasing batch size will have the same effect as decreasing learning rate\nAchieves near-identical model performance on the test set with the same number of training epochs but significantly fewer parameter updates\nTraining cross-entropy\n2.0\n101\n1.5\nLearning rate\n10?\nLearning rate decrease Vs Batch size increase Decaying learning rate\n10¬™\n0.0\n10'\nBatch size\nTest set accuracy\n10*\n10‚Ç¥\n50\n100\nNumber of epochs\n150\nDecaying learning rate\nHybrid\nIncreasing batch size\n50\n100\nNumber of epochs\n150\n200\n200\n0.90\n0.85\n0.80\n0.75\n0\n50\n50\n100\nNumber of epochs\n150|\n200\nDecaying learning rate\nHybrid\nIncreasing batch size\n100\nNumber of epochs\n150\n200\nDecaying learning rate\nHybrid\nIncreasing batch size\n119\nBatch normalization\nInternal covariance shift - change in the distribution of network activations due to change in network parameters during training\nIdea is to reduce internal covariance shift by applying normalization to inputs of each layer\nAchieve fix distribution of inputs at each layer\nNormalization for each training mini-batch .\nBatch normalization enables training with larger learning rates\nReduces the dependence of gradients on the scale of the parameters\nFaster convergence and better generalization\n120\nInput: Values of x over a mini-batch: B = {x1...m};\nParameters to be learned: v, B\nOutput: {yi = BNy, (xi)}\nm\nExi\nMBI\nm\nBatch normalization\nm\ni=1\n// mini-batch mean\nWhy this step ?\n121\nBest practices when benchmarking distributed deep learning systems\nSystems under comparison should train to same accuracy\nAccuracy should be reported on sufficiently large test set\nCompute to communication ratio can vary widely for different neural networks. Using a neural network with high compute to communication ratio can hide the ills of an inferior distributed Deep Learning system.\na sub-optimal communication algorithm or low bandwidth interconnect will not matter that much\nComputation time for one Deep Learning iteration can vary by up to 50% when different Deep Learning frameworks are being used. This increases the compute to communication ratio and gives the inferior distributed Deep Learning system an unfair uplift to the scaling efficiency.\n122\nBest practices when benchmarking distributed deep learning systems\nA slower GPU increases the compute to communication ratio and again gives the inferior distributed Deep Learning system an unfair uplift to the scaling efficiency.\nNvidia P100 GPUs are approximately 3X faster than Nvidia K40 GPUs.\nWhen evaluating the communication algorithm and the interconnect capability of a Deep Learning system, it is important to use a high performance GPU.\nCommunication overhead is the run time of one iteration when distributed over N GPUs minus the run time of one iteration on a single GPU.\nIncludes the communication latency and the time it takes to send the message (gradients) among the GPUs.\nCommunication overhead gives an indication of the quality of the communication algorithm and the interconnect bandwidth.\n123\nDL Pipelining\nPipelining approach:\nSplit layers among compute engines\nEach minibatch b (or sample s) goes from one compute engine to the next one: no need to wait for next one to exit the pipeline\nIs a form of Model Parallelism\nPipelining performance\nIdeal pipelining speedup (number of pipeline stages)\nùëÜ_ùë°ùëñùëöùëí ùë†ùë°ùëéùëîùëí\tùë°ùëñùëöùëí = ùë°ùëñùëöùëí\tùë§ùëñùë°‚Ñéùëúùë¢ùë°\tùëùùëñùëùùëíùëôùëñùëõùëí ùëõùë¢ùëöùëèùëíùëü\tùëúùëì\tùëùùëñùëùùëíùëôùëñùëõùëí\tùë†ùë°ùëéùëîùëíùë†\nSpeedup is higher for deeper networks\nIdeal pipelining never reached because of 'bubbles' that cause idle CPUs\nSGD pipeline bubble:\nBefore weights update, all batches need to have completed forward (otherwise accept staleness )\ntime\nNon-pipelined execution\nùë°\n&\nùë°\n$\nùë°\n'\nùë°\n(\nùë°\n)\nùë°\n*\nùë°\n+\nùë°\n,\nGPU\n1\n(L1)\nb\n2\nb1\nGPU\n2\n(L2)\nb\n2\nb1\nGPU\n3\n(L3)\nb\n2\nb1\nGPU\n4\n(L4)\nb\n2\nb1\nL1\nL2\nL3\nL4\nPipelined execution\nùë°\n&\nùë°\n$\nùë°\n'\nùë°\n(\nùë°\n)\nùë°\n*\nùë°\n+\nùë°\n,\nGPU\n1\n(L1)\nb\n8\nb\n7\nb\n6\nb\n5\nb4\nb3\nb2\nb1\nGPU\n2\n(L2)\nb\n8\nb\n7\nb\n6\nb5\nb4\nb3\nb2\nGPU\n3\n(L3)\nb\n8\nb\n7\nb6\nb5\nb4\nb3\nGPU\n4\n(L4)\nb\n8\nb7\nb6\nb5\nb4\nL1\nL2\nL3\nL4\ntime\nHPML\n124",
  "tables": [
    {
      "rows": 0,
      "content": "RLHF | Introduction | Concept : Human feedback guides reinforcement learning. | Purpose : Align LLMs with human values/preferences. | Fine-Tuning Methods | Instruction Fine-Tuning : Tailor LLMs to follow specific instructions. | Path Methods : Guide learning based on human feedback. | Challenges in RLHF | Toxicity/Misinformation : Mitigate harmful content. | HumanAlignment : Ensure helpful, honest, harmless models. | RLHF Process | Reward Model Training : Evaluate outputs, assign rewards. | Automated Labelers : Replace human labelers, select completions. | LLM Updating : Align models with human feedback. | Proximal Policy Optimization (PPO) | PPO Overview : Fine-tuning via reinforcement learning. | RLHFApplication : Iterative updates, maximize rewards. | Constitutional AI | Concept : Ethical AI alignment with human values. | Role in RLHF : Ensure adherence to societal norms, trust, safety."
    },
    {
      "rows": 0,
      "content": "false negative (fn) | Positive | Negative | true positive (tp) | true negative (tn) | false positive (fp) | True value"
    },
    {
      "rows": 0,
      "content": "Network technology | Bandwidth [Gb/s] | Latency [us] | 10GigE | 10 | 4 | 40GigE | 40 | 4 | IB EDR | 100 | 1 | NVLink | > 400 | 0.1-0.2"
    },
    {
      "rows": 0,
      "content": "Work | Batch size | Processor | DL Library | Interconnect | Training Time | Top-1 Accurac y | Scaling Efficiency | He et al | 256 | Tesla P100 x8 | Caffe | 29 hrs | 75.3% | Goyal et al (Facebook) | 8K | Tesla P100 x256 | Caffe2 | 50 Gbit Ethernet | 60 mins | 76.3% | ~90% | Cho et al (IBM) | 8K | Tesla P100 x256 | Caffe | Infiniband | 50 mins | 75.01% | 95% | Smith et al | 8K √† 16K | Full TPU Pod | Tensorflow | 30 mins | 76.1% | Akiba et al | 32K | Tesla P100 x1024 | Chainer | Infiniband FDR | 15 mins | 74.9% | 80% | Jia et al | 64K | Tesla P40 x2048 | Tensorflow | 100 Gbit Ethernet | 6.6 mins | 75.8% | 87.9% | Ying et al | 32K | TPU v3 x1024 | Tensorflow | 2.2 mins | 76.3% | Ying et al | 64K | TPU v3 x1024 | Tensorflow | 1.8 mins | 75.2% | Mikami et al | 54K | Tesla V100 x3456 | NNL | Infiniband EDR x2 | 2.0 mins | 75.29% | 84.75%"
    },
    {
      "rows": 0,
      "content": "Non-pipelined execution b 2 time | ùë° & | ùë° $ | ùë° ' | ùë° ( | ùë° ) | ùë° * | ùë° + | ùë° , | GPU 1 (L1) L1 | b 2 | b1 | GPU 2 L2 | b 2 | b1 | GPU 3 (L3) L4 | b 2 | b1 | 4 | b1 | Pipelined execution | ùë° & | ùë° $ | ùë° ' | ùë° ( | ùë° ) | ùë° * | ùë° + | ùë° , | GPU 1 (L1) L1 | b 8 | b 7 | b 6 | b 5 | b4 | b3 | b2 | b1 | GPU 2 (L2) L2 L3 | b 8 | b 7 | b 6 | b5 | b4 | b3 | b2 | GPU 3 (L3) L4 | b 8 | b 7 | b6 | b5 | b4 | b3 | GPU 4 (L4) | b 8 | b7 | b6 | b5 | b4"
    }
  ],
  "metadata": {
    "email_id": "19aadaf0f535cdc9",
    "attachment_type": ".pdf",
    "original_filename": "Lecture-1-nyu.pdf",
    "content_type": "document"
  }
}